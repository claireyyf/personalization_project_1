{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalization Project 1 Report\n",
    "\n",
    "\n",
    "**Jessie Du** (yd2464, @chopperase), **Claire Feng** (yf2476, @claireyyf), **Zijun Huang** (zh2356, @zijunh19), **Weisi Yan** (wy2320, @weisiyan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction \n",
    "\n",
    "Suppose the digital media company we work at owns a website that aggregates all kinds of information related to movies. We are asked to build a recommendation system for movies based on a dataset containing user ratings, and the goal is to attract more users and increase profit for the company. \n",
    "\n",
    "Then, in building this recommendation system, our objectives are to recommend movies that are most likely to raise users’ interest and to attract them to spend more time and share more information on our website. \n",
    "\n",
    "We also want to create a sense of community on the platform by recommending movies to users based on people with similar taste (‘people like you also watched…’). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset (How we sample our data)\n",
    "\n",
    "a. **Recent 5 years**: set 5-year as a window during which we assume people’s taste in movie does not change much. This range also guarantees a relatively wide range of choices for our algorithms. \n",
    "\n",
    "b. **Movies with at least 20 ratings over the past 5 years**: to ensure data density so that movies included in the smaller sample still have enough ratings to be split into training and testing set. Also, given the size of the original dataset (and the fact that some movies have been rated 10,000+ times), if one movie has been rated for less than 20 times over the five-year period, there is a high possibility that the movie is either too niche or too old - or too bad, all of which are cases we do not want to recommend to users at the moment.  \n",
    "\n",
    "c. **Users with at least 20 ratings**: to stay consistent with the original dataset, in which each user rated at least 20 movies. This filter also served to ensure that we are only referencing users who had been active in the past five years. \n",
    "\n",
    "d. **Keep 30% of movies rated by each user**: to further shrink the smaller, sample data in favor of expediting the model-building process. We tried different parameters - 100%, 50%, 30%, and 10%. As we went down the percentage, there was not much change in the distribution of data over the first three quantiles, but the variance kept decreasing. 30% was chosen as the final parameter because it was the smallest percentage that still caused an effective shrinkage in both data size and variance. \n",
    "\n",
    "e. “To reduce the dimensionality of the data set, and avoid running into “memory error”, we will filter out rarely rated books and rarely rating users.” (https://towardsdatascience.com/building-and-testing-recommender-systems-with-surprise-step-by-step-d4ba702ef80b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neighborhood-based model:\t\n",
    "\n",
    "## 3.1 Model setup \n",
    "\n",
    "\n",
    "## 3.2 Model evaluation\n",
    "\n",
    "## 3.3 Model exploration\n",
    "\n",
    "## 3.4 Business value\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Matrix Factorization\n",
    "\n",
    "## 4.1 Model setup\n",
    "\n",
    "From the current dataset, we randomly selected 5,000 users and 500 movies and use the corresponding data as our sample - we came up with a sample of 4536 users and 496 movies. We used the SVD algorithm from scipy.sparse.linalg package to implement the model and took 50 as default value for latent dimension. \n",
    "\n",
    "Our recommender selected the top 10 movies that have the highest predicted ratings and are not rated in the train set to recommend to each user. \n",
    "\n",
    "\n",
    "## 4.2 Evaluation methods\n",
    "We implemented a 5-fold cross validation. \n",
    "\n",
    "We first calculated RMSE to assess model performance. However, RMSE doesn’t necessarily tell us how relevant our recommended movies to each user is. Since our business objective is to recommend movies that are relevant and likely to raise users’ interest to explore further, we decided to use precision and recall as our primary accuracy metrics and coverage as secondary metric.\n",
    "\n",
    "\n",
    "## 4.3 Model exploration (hyperparameter & sample size)\n",
    "To tune the hyperparameter, we evaluate performance of the model taking different values [1,2,3,4,5] for latent dimension. From the plots of precisions and recalls, we see that the optimal value for latent dimension is 1 where the model achieves 6.64% precision, 34.1% recall, 7.51% coverage on the train set. We re-train this model on the entire training data and assess its performance on the test set: 10.32% precision, 34.23% recall, 7.22% coverage. \n",
    "\n",
    "      size=5000 users, 500 movies\n",
    "\n",
    "![GitHub](https://github.com/personalization_project_1/images/precision.png)\n",
    "![GitHub](https://github.com/personalization_project_1/images/recall.png)\n",
    "\n",
    "\n",
    "       size=10000 users, 1000 movies\n",
    "\n",
    "![GitHub](https://github.com/personalization_project_1/images/svd_large_precision.png)\n",
    "![GitHub](https://github.com/personalization_project_1/images/svd_large_recall.png)\n",
    "\n",
    "\n",
    "\n",
    "Then, we implement the SVD model on a larger sample. Now we randomly select 10,000 users and 1,000 movies and end up with a sample of 9,855 users and 1,000 movies. We tune the hyperparameter again and find the optimal number of latent dimensions is still 1. The model has 6.38% precision, 22.4% recall, 3.27% coverage on the train set and has 11.5% precision, 23.13% recall, 3.75% coverage on the test set. \n",
    "\n",
    "We can see that overall precision improves while recall and coverage decrease with a larger sample size. For the original small sample, it took 6 minutes to finish the model selection process (hyperparameter tuning); for the larger sample, it took 24 minutes. \n",
    "\n",
    "\n",
    "## 4.4 Business value\n",
    "\n",
    "Our model mainly generates recommendations from very popular movies. Although such recommendations might bore some users, well, these movies are popular because they are liked by most people. Most users would be curious to know the cast, ratings, reviews, etc., or they might leave a rating or review of their own - now we successfully attract them to stay longer on the website and collect more data! \n",
    "\n",
    "\n",
    "## 4.5 Analysis (potential problems)\n",
    "\n",
    "In collaborative filtering, the more ratings data there are available for each user and for each item, the more personalized can the generated recommendations be. However, in our sample, first of all, many users have a small number of ratings; second, popular movies have significantly more ratings than the others. For example, 75% of the movies in our sample have less than 54 ratings but the popular movies have hundreds of ratings. Therefore, the model tends to recommend only from the popular group and the coverage is rather small. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "## 5.1 Business objectives\n",
    "\n",
    "## 5.2 Future improvement\n",
    "\n",
    "As for the next steps for improvement for matrix factorization model, we could try other algorithms besides SVD, such as ALS. Also, we could use distributed computing framework like SparkML to implement the model and increase the speed of factorization calculation. \n",
    "\n",
    "In the current model, we only look at users and movies, while there are other factors that haven’t been taken into consideration, such as genres and tags. Using additional information in the recommendation system can help further personalize the experience for users.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
